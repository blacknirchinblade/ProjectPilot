# AutoCoder: Interview Preparation Guide

This guide contains a comprehensive list of potential interview questions about the AutoCoder project. It is designed to give you a deep understanding of the system's architecture, design choices, and technical implementation.

---

## Section 1: High-Level System Design

### ‚ùì Question 1: Can you describe the high-level architecture of the AutoCoder system?

**üîë Key Points to Cover:**
-   It's a **multi-agent system** designed for automated software development.
-   The architecture is **modular**, with a clear separation of concerns between the UI, the core logic, and the agents.
-   The main components are the **Streamlit Web UI**, a central **Orchestrator**, a collection of specialized **AI Agents**, and a suite of **Automation Tools**.
-   Communication is **centralized** through the Orchestrator, which manages the workflow and delegates tasks.

**üí° Expert Answer:**
"The AutoCoder system is built on a modular, multi-agent architecture. At the top layer, we have a Streamlit web interface that serves as the primary user entry point. The core of the system is a central Orchestrator, which acts as the 'brain', managing the entire project generation workflow.

The Orchestrator delegates tasks to a team of specialized AI agents, each an expert in a specific domain like software architecture, coding, or testing. These agents don't communicate directly; instead, they report back to the Orchestrator, which ensures a predictable, task-driven workflow. Finally, the agents leverage a set of non-AI Automation Tools to perform concrete actions like file I/O and running shell commands. This design allows for a clean separation of concerns and makes the system highly extensible."

**üîÑ Follow-up Questions:**
-   *Why was a multi-agent approach chosen over a single, monolithic AI model?*
-   *What are the advantages and disadvantages of the centralized Orchestrator model?*
-   *How would you handle communication if you needed more complex, real-time collaboration between agents?*

---

### ‚ùì Question 2: Walk me through the data flow for a typical project generation request.

**üîë Key Points to Cover:**
-   The flow starts with the user's description in the UI.
-   It moves to the Orchestrator, which initiates the "Clarification Phase."
-   After clarification, the ArchitectAgent designs the project structure.
-   The Orchestrator then enters an "Auto-Iteration Loop" (Code -> Review -> Test).
-   The loop continues until a target quality score is met.
-   The final project is then made available for download.

**üí° Expert Answer:**
"The workflow begins when a user submits a project description through the Streamlit UI. This request is sent to the Orchestrator, which first tasks the ClarificationAgent to generate questions for the user. Once the user's answers are received, the Orchestrator passes the refined specification to the DynamicArchitectAgent, which designs the complete file structure.

From there, the system enters its main auto-iteration loop. The Orchestrator iterates through the architectural plan, tasking the CodingAgent to write the code for each file. After a file is written, it's passed to the ReviewOrchestrator and the TestingAgent for quality assurance. The results are aggregated into a quality score. If the score is below our target threshold, the Orchestrator provides the feedback to the CodingAgent and tasks it to improve the code. This 'Code, Review, Test' loop continues until the quality target is met, at which point the process is complete and the user can download the project."

**üîÑ Follow-up Questions:**
-   *How is the "quality score" calculated?*
-   *What happens if the system gets stuck in an infinite loop of revisions? How would you detect and prevent that?*
-   *How does the system maintain state between these different agent invocations?*

---

## Section 2: The Agent Framework

### ‚ùì Question 3: What is the role of the `BaseAgent`, and why is it important?

**üîë Key Points to Cover:**
-   It's an **abstract base class** that all other agents inherit from.
-   It enforces a **consistent interface** for all agents, primarily through the `execute_task` method.
-   It provides **common functionality**, such as integration with the LLM client and the prompt manager.
-   This design makes the system **pluggable and extensible**.

**üí° Expert Answer:**
"The `BaseAgent` is an abstract base class that serves as the foundation for our entire agent framework. Its primary purpose is to enforce a consistent interface across all specialized agents. Every agent must implement an `execute_task` method, which provides a standard entry point for the Orchestrator to delegate tasks.

Additionally, the `BaseAgent` provides common, shared functionality. For example, it handles the initialization of the LLM client and the prompt manager, so we don't have to duplicate that logic in every agent. This object-oriented design is crucial because it makes the system highly extensible. To add a new capability, we can simply create a new class that inherits from `BaseAgent`, implement the `execute_task` method, and the Orchestrator can immediately start using it without any other changes."

**üîÑ Follow-up Questions:**
-   *What other common methods or properties might you consider adding to the `BaseAgent`?*
-   *How would you handle a situation where a new agent needs a completely different method signature for its tasks?*

---

### ‚ùì Question 4: Explain the "Auto-Iteration" workflow and the roles of the CodingAgent, ReviewOrchestrator, and TestingAgent in it.

**üîë Key Points to Cover:**
-   It's a loop designed to **iteratively improve code quality**.
-   **CodingAgent**: Writes the initial version of the code.
-   **ReviewOrchestrator**: Manages a team of sub-reviewers to analyze the code for issues (readability, performance, etc.) and produces a score and feedback.
-   **TestingAgent**: Generates and runs `pytest` unit tests to check for functional correctness.
-   The feedback from the review and test results is fed back to the CodingAgent for the next iteration.

**üí° Expert Answer:**
"The auto-iteration workflow is our core quality assurance mechanism. Instead of just generating code once and hoping it's correct, we treat the first draft as just the beginning.

Here's how it works: First, the **CodingAgent** generates the Python code for a specific file based on the architect's plan. This code is then passed to two other agents in parallel. The **ReviewOrchestrator** analyzes the code for non-functional issues like readability, performance, and security, producing a quality score and a list of suggested improvements. At the same time, the **TestingAgent** generates `pytest` unit tests and executes them to check for bugs and logical errors.

The Orchestrator then aggregates the feedback from both the review and the test results. If the quality score is below our target or if any tests fail, this feedback is passed back to the CodingAgent. The agent is then tasked to 'regenerate the code, taking this feedback into account.' This loop continues until the code is both well-written and functionally correct."

**üîÑ Follow-up Questions:**
-   *How do you prevent the agent from making the same mistake over and over again?*
-   *Could this process be made more efficient? For example, could the review and testing happen at a more granular level?*

---

## Section 3: Technical Deep Dive

### ‚ùì Question 5: The system uses `asyncio` in some places but not others. Why? And how is the event loop managed in the Streamlit environment?

**üîë Key Points to Cover:**
-   `asyncio` is used for tasks that are **I/O-bound**, primarily the calls to the external LLM API. This allows the system to make multiple API calls concurrently, significantly speeding up the process.
-   CPU-bound tasks (like file I/O or data processing) are generally kept synchronous.
-   Streamlit has its own way of managing state and doesn't have a native, persistent asyncio event loop.
-   The code handles this by creating a **new event loop for each async operation** (`asyncio.new_event_loop()`, `asyncio.set_event_loop()`, `loop.run_until_complete()`).

**üí° Expert Answer:**
"We use `asyncio` strategically for I/O-bound operations, with the most important one being the calls to the Gemini LLM API. Generating a response from the LLM can take several seconds. By using `asyncio`, we can fire off multiple requests to the LLM concurrently‚Äîfor example, when the ReviewOrchestrator is running all its sub-reviewers in parallel. This provides a significant performance improvement over making those calls sequentially.

However, we have to be careful because Streamlit runs on a standard synchronous server model and doesn't have a persistent asyncio event loop. To bridge this gap, we adopt a common pattern for integrating async code into a sync environment. For each user action that triggers an async function, we create a new event loop on the fly using `asyncio.new_event_loop()`, set it as the current loop, run our async task to completion with `loop.run_until_complete()`, and then close the loop. It's not as efficient as a fully async framework, but it's a robust and reliable way to get the benefits of `asyncio` within a Streamlit application."

**üîÑ Follow-up Questions:**
-   *What are the potential downsides of creating a new event loop for every operation?*
-   *How would the architecture change if you were to build this on a fully asynchronous web framework like FastAPI or Starlette instead of Streamlit?*

---

### ‚ùì Question 6: A `[WinError 206] The filename or extension is too long` error was a major issue. What caused it, and how was it fixed?

**üîë Key Points to Cover:**
-   The error is a **Windows-specific limitation** where file paths cannot exceed ~260 characters.
-   The root cause was a **bug in path construction**. The code was incorrectly concatenating a relative project path with an already absolute path, leading to a duplicated and overly long path string.
-   The fix was to **enforce a simple, short name** for the virtual environment directory ("venv") instead of using the long, auto-generated project name. This kept the overall path length well under the Windows limit.

**üí° Expert Answer:**
"That was a critical bug that specifically affected Windows users. The `[WinError 206]` error occurs because the Windows operating system has a maximum path length limit of around 260 characters.

The root cause was a subtle bug in our setup automation logic. The system was generating a long, unique name for each project's output directory, like `output/my_ai_project_20251022...`. It was then trying to create a Conda environment *inside* that directory with the *same long name*. This, combined with the deeply nested `site-packages` structure of Python environments, resulted in file paths that exceeded the Windows limit.

The solution was to decouple the environment name from the project name. I modified the `SetupAutomationTool` to always use the simple, predictable name `"venv"` for the virtual environment directory, regardless of the project's name. This ensures that the paths generated during the environment creation are always short and well within the operating system's limits, making the tool robust on Windows."

**üîÑ Follow-up Questions:**
-   *Are there other ways to solve this problem on Windows (e.g., registry changes)? Why was the chosen solution the best one for this application?*
-   *How would you design a testing strategy to catch platform-specific bugs like this in the future?*
